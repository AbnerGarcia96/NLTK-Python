{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostración: NLTK Para el Procesamiento del Lenguaje Natural\n",
    "\n",
    "Muchas gracias por asistir al taller de Procesamiento del Lenguaje Natural.\n",
    "\n",
    "A continuación, veremos algunos de los distintos métodos que tiene la librería NLTK de Python para procesar y transformar cadenas de texto. Esto aplica para oraciones simples y para cuerpos complejos de texto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Tokenize: Tokenizar palabras dentro de un texto\n",
    "\n",
    "Este método se encarga de separar el texto por palabras, incluyendo los signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto1 = \"Hi, how are you?\"\n",
    "word_tokenize(texto1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto2 = \"Hola, ¿qué tal?\"\n",
    "word_tokenize(texto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Utilizando otro tokenizador para las palabras en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizador = ToktokTokenizer()\n",
    "tokenizador.tokenize(texto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Sentence Tokenize: Tokenizar oraciones dentro de un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "texto3 = \"Hola, este texto es una prueba para tokenizar oraciones. Hoy está nublado, el clima está agradable. ¿No les gusta este clima a ustedes?\"\n",
    "\n",
    "sent_tokenize(texto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. RegExp Tokenize: Tokenizar oraciones utilizando expresiones regulares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizador_regex = RegexpTokenizer('[a-zA-ZñÑ]+')\n",
    "\n",
    "tokenizador_regex.tokenize(texto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Palabras vacías\n",
    "\n",
    "Estas son palabras que no aportan información significativa a las oraciones, por lo general son artículos, pronombres, preposiciones, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = []\n",
    "\n",
    "for palabra in tokenizador.tokenize(texto3):\n",
    "    if palabra not in stopwords.words():\n",
    "        palabras.append(palabra)\n",
    "\n",
    "palabras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
